import chainlit as cl
from llama_index.core import StorageContext, load_index_from_storage, Settings
# On remplace l'import OpenAI par Groq pour √©viter l'erreur de validation du nom du mod√®le
from llama_index.llms.groq import Groq 
from llama_index.embeddings.huggingface import HuggingFaceEmbedding 
import os
from dotenv import load_dotenv

# Charger les variables d'environnement (.env)
load_dotenv()

# --- CONFIGURATION CRITIQUE ---

# 1. EMBEDDING : IDENTIQUE √† build_index.py (BAAI/bge-m3)
# Cela permet de lire correctement les vecteurs cr√©√©s pr√©c√©demment.
print("Chargement du mod√®le d'embedding...")
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3")

# 2. LLM (Groq) : Utilisation du connecteur officiel Groq
# Cela contourne l'erreur "Unknown model" de la classe OpenAI standard.
Settings.llm = Groq(
    model="llama-3.1-8b-instant",              # Mod√®le Llama 3 sur Groq
    temperature=0.1,                     # Faible temp√©rature pour la pr√©cision
    api_key=os.getenv("OPENAI_API_KEY")  # On r√©cup√®re ta cl√© gsk_ ici
)

# Message de disclaimer obligatoire (Crit√®re Interface)
DISCLAIMER = """
‚ö†Ô∏è **AVERTISSEMENT IMPORTANT** ‚ö†Ô∏è
Ce chatbot est un prototype informatif destin√© √† un organisme public. 
**Il ne remplace en aucun cas un avis m√©dical professionnel.**
En cas de doute sur votre √©tat de sant√©, consultez imm√©diatement un m√©decin.
"""

@cl.on_chat_start
async def start():
    """Cette fonction s'ex√©cute au d√©marrage d'une nouvelle session utilisateur."""
    try:
        # Chargement de l'index persistant cr√©√© par build_index.py
        storage_context = StorageContext.from_defaults(persist_dir="./index_storage")
        index = load_index_from_storage(storage_context)
        
        # Cr√©ation du moteur de requ√™te (Retriever + LLM)
        query_engine = index.as_query_engine(
            streaming=True, 
            similarity_top_k=3  # R√©cup√®re les 3 morceaux les plus pertinents
        )
        
        # On stocke le moteur dans la session pour le r√©utiliser √† chaque message
        cl.user_session.set("query_engine", query_engine)

        # Envoi du message d'accueil avec le disclaimer
        await cl.Message(content=f"Bonjour ! Je suis votre assistant virtuel sur le diab√®te (Propuls√© par Groq).\n{DISCLAIMER}").send()
        
    except Exception as e:
        error_msg = (
            f"Erreur critique au chargement : {e}. \n\n"
            "1. Avez-vous lanc√© 'pip install llama-index-llms-groq' ?\n"
            "2. Avez-vous bien lanc√© 'python build_index.py' avant ?"
        )
        await cl.Message(content=error_msg).send()

@cl.on_message
async def main(message: cl.Message):
    """Cette fonction s'ex√©cute √† chaque fois que l'utilisateur envoie un message."""
    
    query_engine = cl.user_session.get("query_engine")
    
    # Prompt syst√®me renfor√ßant le r√¥le et la s√©curit√©
    prompt_complet = (
        "Tu es un assistant expert en diab√®te pour un organisme public de sant√©. "
        "Tu dois r√©pondre en fran√ßais de mani√®re claire et p√©dagogique. "
        "Utilise EXCLUSIVEMENT le contexte fourni ci-dessous pour r√©pondre. "
        "Si la r√©ponse ne se trouve pas dans le contexte, dis poliment que tu ne sais pas, n'invente rien. "
        "Reste empathique mais professionnel et factuel.\n\n"
        f"Question de l'utilisateur : {message.content}"
    )

    msg = cl.Message(content="")
    
    try:
        # Appel au moteur RAG
        response = query_engine.query(prompt_complet)

        # Diffusion de la r√©ponse (Streaming) mot par mot
        for token in response.response_gen:
            await msg.stream_token(token)

        # --- GESTION DES SOURCES (Crit√®re d'√©valuation : Fid√©lit√© & Rigueur) ---
        if response.source_nodes:
            sources_text = "\n\n---Sources utilis√©es---"
            unique_sources = set()
            
            for node in response.source_nodes:
                # On r√©cup√®re la m√©tadonn√©e "source" d√©finie dans build_index.py
                src = node.metadata.get('source', 'Source inconnue')
                topic = node.metadata.get('topic', 'G√©n√©ral')
                
                # On cr√©e une chaine propre "Source (Sujet)"
                source_entry = f"{src} (Th√®me : {topic})"
                unique_sources.add(source_entry)
            
            for src in unique_sources:
                sources_text += f"\nüìö {src}"
            
            await msg.stream_token(sources_text)
        
        await msg.send()
        
    except Exception as e:
        await cl.Message(content=f"Une erreur est survenue lors de la g√©n√©ration de la r√©ponse : {e}").send()